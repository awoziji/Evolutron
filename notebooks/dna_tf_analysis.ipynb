{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device 1 failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "os.chdir('../CoMET')\n",
    "\n",
    "import evolutron.networks as nets\n",
    "from evolutron.motifs import motif_extraction\n",
    "from evolutron.tools import load_dataset, none2str, Handle\n",
    "from evolutron.engine import DeepTrainer\n",
    "\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data and model\n",
    "data_id = 'dnabind'\n",
    "padded = True\n",
    "mode = 'family'\n",
    "\n",
    "# Architecture\n",
    "filters = 300\n",
    "filter_length = 50\n",
    "conv = 1\n",
    "fc = 1\n",
    "\n",
    "# Training\n",
    "num_epochs = 200\n",
    "batch_size = 50\n",
    "optimizer = 'nadam'\n",
    "rate = .005\n",
    "validate = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 6703\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(data_id, padded=padded, codes=True)\n",
    "x_data, y_data = dataset\n",
    "if type(x_data) == np.ndarray:\n",
    "    input_shape = x_data[0].shape\n",
    "elif type(x_data) == list:\n",
    "    input_shape = (None, x_data[0].shape[1])\n",
    "else:\n",
    "    raise TypeError('Something went wrong with the dataset type')\n",
    "\n",
    "y_data = to_categorical(y_data)\n",
    "output_dim = y_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 399)\n",
      "(None, None)\n"
     ]
    }
   ],
   "source": [
    "net_arch = nets.DeepCoFAM.from_options(input_shape,\n",
    "                                               output_dim,\n",
    "                                               n_conv_layers=conv,\n",
    "                                               n_fc_layers=fc,\n",
    "                                               n_filters=filters,\n",
    "                                               filter_length=filter_length)\n",
    "conv_net = DeepTrainer(net_arch)\n",
    "conv_net.compile(optimizer=optimizer, lr=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network has 510699 trainable parameters\n",
      "  #  Name            Shape       Parameters\n",
      "---  --------------  --------  ------------\n",
      "  0  aa_seq          1407x20              0\n",
      "  1  Conv1           1407x300        300300\n",
      "  2  maxpooling1d_1  1x300                0\n",
      "  3  flatten_1       300                  0\n",
      "  4  FCEnc1          300              90300\n",
      "  5  Classifier      399             120099\n"
     ]
    }
   ],
   "source": [
    "conv_net.display_network_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5362, 1407, 20) (5362, 399)\n",
      "Train on 5362 samples, validate on 1341 samples\n",
      "Epoch 1/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 6.0344 - mean_cat_acc: 0.6256Epoch 00000: val_loss improved from inf to 6.10538, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 6.0359 - mean_cat_acc: 0.6255 - val_loss: 6.1054 - val_mean_cat_acc: 0.6219\n",
      "Epoch 2/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 6.0131 - mean_cat_acc: 0.6269Epoch 00001: val_loss improved from 6.10538 to 6.08803, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 6.0116 - mean_cat_acc: 0.6270 - val_loss: 6.0880 - val_mean_cat_acc: 0.6227\n",
      "Epoch 3/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.8638 - mean_cat_acc: 0.6363Epoch 00002: val_loss improved from 6.08803 to 6.02093, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.8658 - mean_cat_acc: 0.6361 - val_loss: 6.0209 - val_mean_cat_acc: 0.6264\n",
      "Epoch 4/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.8328 - mean_cat_acc: 0.6381Epoch 00003: val_loss improved from 6.02093 to 6.01571, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.8348 - mean_cat_acc: 0.6380 - val_loss: 6.0157 - val_mean_cat_acc: 0.6264\n",
      "Epoch 5/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.8047 - mean_cat_acc: 0.6402Epoch 00004: val_loss improved from 6.01571 to 5.99368, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.8038 - mean_cat_acc: 0.6402 - val_loss: 5.9937 - val_mean_cat_acc: 0.6286\n",
      "Epoch 6/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7907 - mean_cat_acc: 0.6407Epoch 00005: val_loss improved from 5.99368 to 5.98208, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.7958 - mean_cat_acc: 0.6404 - val_loss: 5.9821 - val_mean_cat_acc: 0.6294\n",
      "Epoch 7/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7906 - mean_cat_acc: 0.6407Epoch 00006: val_loss improved from 5.98208 to 5.98164, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.7956 - mean_cat_acc: 0.6404 - val_loss: 5.9816 - val_mean_cat_acc: 0.6294\n",
      "Epoch 8/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.8026 - mean_cat_acc: 0.6400Epoch 00007: val_loss improved from 5.98164 to 5.98159, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.7956 - mean_cat_acc: 0.6404 - val_loss: 5.9816 - val_mean_cat_acc: 0.6294\n",
      "Epoch 9/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7941 - mean_cat_acc: 0.6406Epoch 00008: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.7931 - mean_cat_acc: 0.6406 - val_loss: 5.9846 - val_mean_cat_acc: 0.6286\n",
      "Epoch 10/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7905 - mean_cat_acc: 0.6407Epoch 00009: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.7926 - mean_cat_acc: 0.6406 - val_loss: 5.9846 - val_mean_cat_acc: 0.6286\n",
      "Epoch 11/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7995 - mean_cat_acc: 0.6402Epoch 00010: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.7926 - mean_cat_acc: 0.6406 - val_loss: 5.9843 - val_mean_cat_acc: 0.6286\n",
      "Epoch 12/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7482 - mean_cat_acc: 0.6436Epoch 00011: val_loss improved from 5.98159 to 5.94624, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.7503 - mean_cat_acc: 0.6434 - val_loss: 5.9462 - val_mean_cat_acc: 0.6316\n",
      "Epoch 13/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7370 - mean_cat_acc: 0.6441Epoch 00012: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.7272 - mean_cat_acc: 0.6447 - val_loss: 5.9528 - val_mean_cat_acc: 0.6309\n",
      "Epoch 14/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7334 - mean_cat_acc: 0.6443Epoch 00013: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.7265 - mean_cat_acc: 0.6447 - val_loss: 5.9496 - val_mean_cat_acc: 0.6309\n",
      "Epoch 15/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7303 - mean_cat_acc: 0.6445Epoch 00014: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.7265 - mean_cat_acc: 0.6447 - val_loss: 5.9483 - val_mean_cat_acc: 0.6309\n",
      "Epoch 16/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7212 - mean_cat_acc: 0.6450Epoch 00015: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.7265 - mean_cat_acc: 0.6447 - val_loss: 5.9475 - val_mean_cat_acc: 0.6309\n",
      "Epoch 17/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.7060 - mean_cat_acc: 0.6462Epoch 00016: val_loss improved from 5.94624 to 5.84672, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.6992 - mean_cat_acc: 0.6466 - val_loss: 5.8467 - val_mean_cat_acc: 0.6376\n",
      "Epoch 18/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.6080 - mean_cat_acc: 0.6521Epoch 00017: val_loss improved from 5.84672 to 5.82913, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.6074 - mean_cat_acc: 0.6522 - val_loss: 5.8291 - val_mean_cat_acc: 0.6383\n",
      "Epoch 19/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.6098 - mean_cat_acc: 0.6520Epoch 00018: val_loss improved from 5.82913 to 5.82673, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.6063 - mean_cat_acc: 0.6522 - val_loss: 5.8267 - val_mean_cat_acc: 0.6383\n",
      "Epoch 20/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.6128 - mean_cat_acc: 0.6518Epoch 00019: val_loss improved from 5.82673 to 5.82544, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.6063 - mean_cat_acc: 0.6522 - val_loss: 5.8254 - val_mean_cat_acc: 0.6383\n",
      "Epoch 21/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.6037 - mean_cat_acc: 0.6523Epoch 00020: val_loss improved from 5.82544 to 5.82455, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.6062 - mean_cat_acc: 0.6522 - val_loss: 5.8246 - val_mean_cat_acc: 0.6391\n",
      "Epoch 22/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.6128 - mean_cat_acc: 0.6518Epoch 00021: val_loss improved from 5.82455 to 5.82398, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.6062 - mean_cat_acc: 0.6522 - val_loss: 5.8240 - val_mean_cat_acc: 0.6391\n",
      "Epoch 23/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.6102 - mean_cat_acc: 0.6520Epoch 00022: val_loss improved from 5.82398 to 5.79020, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.6037 - mean_cat_acc: 0.6524 - val_loss: 5.7902 - val_mean_cat_acc: 0.6413\n",
      "Epoch 24/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5900 - mean_cat_acc: 0.6533Epoch 00023: val_loss improved from 5.79020 to 5.77849, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5926 - mean_cat_acc: 0.6531 - val_loss: 5.7785 - val_mean_cat_acc: 0.6421\n",
      "Epoch 25/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5871 - mean_cat_acc: 0.6535Epoch 00024: val_loss improved from 5.77849 to 5.77741, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5836 - mean_cat_acc: 0.6537 - val_loss: 5.7774 - val_mean_cat_acc: 0.6413\n",
      "Epoch 26/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5767 - mean_cat_acc: 0.6540Epoch 00025: val_loss improved from 5.77741 to 5.77701, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5822 - mean_cat_acc: 0.6537 - val_loss: 5.7770 - val_mean_cat_acc: 0.6413\n",
      "Epoch 27/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5827 - mean_cat_acc: 0.6536Epoch 00026: val_loss improved from 5.77701 to 5.77662, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5822 - mean_cat_acc: 0.6537 - val_loss: 5.7766 - val_mean_cat_acc: 0.6413\n",
      "Epoch 28/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5857 - mean_cat_acc: 0.6535Epoch 00027: val_loss improved from 5.77662 to 5.77621, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5822 - mean_cat_acc: 0.6537 - val_loss: 5.7762 - val_mean_cat_acc: 0.6413\n",
      "Epoch 29/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5746 - mean_cat_acc: 0.6542Epoch 00028: val_loss improved from 5.77621 to 5.74922, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5742 - mean_cat_acc: 0.6542 - val_loss: 5.7492 - val_mean_cat_acc: 0.6435\n",
      "Epoch 30/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5633 - mean_cat_acc: 0.6550Epoch 00029: val_loss improved from 5.74922 to 5.74350, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5629 - mean_cat_acc: 0.6550 - val_loss: 5.7435 - val_mean_cat_acc: 0.6435\n",
      "Epoch 31/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5649 - mean_cat_acc: 0.6548Epoch 00030: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.5615 - mean_cat_acc: 0.6550 - val_loss: 5.7443 - val_mean_cat_acc: 0.6443\n",
      "Epoch 32/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5676 - mean_cat_acc: 0.6546Epoch 00031: val_loss improved from 5.74350 to 5.74192, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5612 - mean_cat_acc: 0.6550 - val_loss: 5.7419 - val_mean_cat_acc: 0.6435\n",
      "Epoch 33/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5586 - mean_cat_acc: 0.6551Epoch 00032: val_loss improved from 5.74192 to 5.74078, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5611 - mean_cat_acc: 0.6550 - val_loss: 5.7408 - val_mean_cat_acc: 0.6435\n",
      "Epoch 34/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5616 - mean_cat_acc: 0.6550Epoch 00033: val_loss improved from 5.74078 to 5.73987, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5611 - mean_cat_acc: 0.6550 - val_loss: 5.7399 - val_mean_cat_acc: 0.6443\n",
      "Epoch 35/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5585 - mean_cat_acc: 0.6551Epoch 00034: val_loss improved from 5.73987 to 5.73912, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5611 - mean_cat_acc: 0.6550 - val_loss: 5.7391 - val_mean_cat_acc: 0.6443\n",
      "Epoch 36/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5555 - mean_cat_acc: 0.6553Epoch 00035: val_loss improved from 5.73912 to 5.73851, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5611 - mean_cat_acc: 0.6550 - val_loss: 5.7385 - val_mean_cat_acc: 0.6443\n",
      "Epoch 37/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.5370 - mean_cat_acc: 0.6564Epoch 00036: val_loss improved from 5.73851 to 5.56348, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.5337 - mean_cat_acc: 0.6567 - val_loss: 5.5635 - val_mean_cat_acc: 0.6547\n",
      "Epoch 38/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3970 - mean_cat_acc: 0.6654Epoch 00037: val_loss improved from 5.56348 to 5.54384, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.4000 - mean_cat_acc: 0.6652 - val_loss: 5.5438 - val_mean_cat_acc: 0.6570\n",
      "Epoch 39/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3954 - mean_cat_acc: 0.6652Epoch 00038: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.3954 - mean_cat_acc: 0.6652 - val_loss: 5.5721 - val_mean_cat_acc: 0.6547\n",
      "Epoch 40/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3868 - mean_cat_acc: 0.6658Epoch 00039: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.3927 - mean_cat_acc: 0.6654 - val_loss: 5.5567 - val_mean_cat_acc: 0.6562\n",
      "Epoch 41/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3969 - mean_cat_acc: 0.6652Epoch 00040: val_loss improved from 5.54384 to 5.54225, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.3908 - mean_cat_acc: 0.6656 - val_loss: 5.5423 - val_mean_cat_acc: 0.6570\n",
      "Epoch 42/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3985 - mean_cat_acc: 0.6652Epoch 00041: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.3924 - mean_cat_acc: 0.6656 - val_loss: 5.5622 - val_mean_cat_acc: 0.6562\n",
      "Epoch 43/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3553 - mean_cat_acc: 0.6679Epoch 00042: val_loss improved from 5.54225 to 5.49463, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.3584 - mean_cat_acc: 0.6677 - val_loss: 5.4946 - val_mean_cat_acc: 0.6600\n",
      "Epoch 44/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3586 - mean_cat_acc: 0.6675Epoch 00043: val_loss improved from 5.49463 to 5.47575, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.3526 - mean_cat_acc: 0.6678 - val_loss: 5.4757 - val_mean_cat_acc: 0.6607\n",
      "Epoch 45/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3425 - mean_cat_acc: 0.6686Epoch 00044: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.3486 - mean_cat_acc: 0.6682 - val_loss: 5.5013 - val_mean_cat_acc: 0.6585\n",
      "Epoch 46/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3417 - mean_cat_acc: 0.6686Epoch 00045: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.3478 - mean_cat_acc: 0.6682 - val_loss: 5.4848 - val_mean_cat_acc: 0.6600\n",
      "Epoch 47/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3416 - mean_cat_acc: 0.6686Epoch 00046: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.3477 - mean_cat_acc: 0.6682 - val_loss: 5.4829 - val_mean_cat_acc: 0.6607\n",
      "Epoch 48/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3567 - mean_cat_acc: 0.6677Epoch 00047: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.3477 - mean_cat_acc: 0.6682 - val_loss: 5.4821 - val_mean_cat_acc: 0.6607\n",
      "Epoch 49/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3446 - mean_cat_acc: 0.6684Epoch 00048: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.3477 - mean_cat_acc: 0.6682 - val_loss: 5.4809 - val_mean_cat_acc: 0.6607\n",
      "Epoch 50/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3406 - mean_cat_acc: 0.6688Epoch 00049: val_loss did not improve\n",
      "5362/5362 [==============================] - 8s - loss: 5.3437 - mean_cat_acc: 0.6686 - val_loss: 5.5212 - val_mean_cat_acc: 0.6585\n",
      "Epoch 51/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.3052 - mean_cat_acc: 0.6712Epoch 00050: val_loss improved from 5.47575 to 5.46998, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.3054 - mean_cat_acc: 0.6712 - val_loss: 5.4700 - val_mean_cat_acc: 0.6622\n",
      "Epoch 52/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.2730 - mean_cat_acc: 0.6729Epoch 00051: val_loss improved from 5.46998 to 5.44792, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.2793 - mean_cat_acc: 0.6725 - val_loss: 5.4479 - val_mean_cat_acc: 0.6637\n",
      "Epoch 53/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.2755 - mean_cat_acc: 0.6727Epoch 00052: val_loss improved from 5.44792 to 5.44413, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.2787 - mean_cat_acc: 0.6725 - val_loss: 5.4441 - val_mean_cat_acc: 0.6637\n",
      "Epoch 54/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.2810 - mean_cat_acc: 0.6723Epoch 00053: val_loss improved from 5.44413 to 5.42522, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.2782 - mean_cat_acc: 0.6725 - val_loss: 5.4252 - val_mean_cat_acc: 0.6652\n",
      "Epoch 55/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.2448 - mean_cat_acc: 0.6748Epoch 00054: val_loss improved from 5.42522 to 5.41611, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.2481 - mean_cat_acc: 0.6746 - val_loss: 5.4161 - val_mean_cat_acc: 0.6652\n",
      "Epoch 56/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.2394 - mean_cat_acc: 0.6750Epoch 00055: val_loss improved from 5.41611 to 5.41170, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.2427 - mean_cat_acc: 0.6747 - val_loss: 5.4117 - val_mean_cat_acc: 0.6652\n",
      "Epoch 57/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.2483 - mean_cat_acc: 0.6744Epoch 00056: val_loss improved from 5.41170 to 5.40982, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.2426 - mean_cat_acc: 0.6747 - val_loss: 5.4098 - val_mean_cat_acc: 0.6652\n",
      "Epoch 58/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.2453 - mean_cat_acc: 0.6746Epoch 00057: val_loss improved from 5.40982 to 5.40842, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.2426 - mean_cat_acc: 0.6747 - val_loss: 5.4084 - val_mean_cat_acc: 0.6652\n",
      "Epoch 59/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.2180 - mean_cat_acc: 0.6763Epoch 00058: val_loss improved from 5.40842 to 5.37639, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.2274 - mean_cat_acc: 0.6757 - val_loss: 5.3764 - val_mean_cat_acc: 0.6682\n",
      "Epoch 60/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.1722 - mean_cat_acc: 0.6793Epoch 00059: val_loss improved from 5.37639 to 5.35052, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.1696 - mean_cat_acc: 0.6794 - val_loss: 5.3505 - val_mean_cat_acc: 0.6696\n",
      "Epoch 61/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.1675 - mean_cat_acc: 0.6794Epoch 00060: val_loss improved from 5.35052 to 5.33509, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.1650 - mean_cat_acc: 0.6796 - val_loss: 5.3351 - val_mean_cat_acc: 0.6704\n",
      "Epoch 62/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.1448 - mean_cat_acc: 0.6809Epoch 00061: val_loss improved from 5.33509 to 5.31380, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.1453 - mean_cat_acc: 0.6809 - val_loss: 5.3138 - val_mean_cat_acc: 0.6719\n",
      "Epoch 63/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.1281 - mean_cat_acc: 0.6819Epoch 00062: val_loss improved from 5.31380 to 5.30772, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.1287 - mean_cat_acc: 0.6818 - val_loss: 5.3077 - val_mean_cat_acc: 0.6719\n",
      "Epoch 64/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.1279 - mean_cat_acc: 0.6819Epoch 00063: val_loss improved from 5.30772 to 5.30590, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.1284 - mean_cat_acc: 0.6818 - val_loss: 5.3059 - val_mean_cat_acc: 0.6719\n",
      "Epoch 65/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.1248 - mean_cat_acc: 0.6821Epoch 00064: val_loss improved from 5.30590 to 5.30389, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.1284 - mean_cat_acc: 0.6818 - val_loss: 5.3039 - val_mean_cat_acc: 0.6719\n",
      "Epoch 66/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.1338 - mean_cat_acc: 0.6815Epoch 00065: val_loss improved from 5.30389 to 5.30268, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.1283 - mean_cat_acc: 0.6818 - val_loss: 5.3027 - val_mean_cat_acc: 0.6719\n",
      "Epoch 67/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.1278 - mean_cat_acc: 0.6819Epoch 00066: val_loss improved from 5.30268 to 5.30166, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.1283 - mean_cat_acc: 0.6818 - val_loss: 5.3017 - val_mean_cat_acc: 0.6719\n",
      "Epoch 68/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.1312 - mean_cat_acc: 0.6817Epoch 00067: val_loss improved from 5.30166 to 5.25274, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.1257 - mean_cat_acc: 0.6820 - val_loss: 5.2527 - val_mean_cat_acc: 0.6756\n",
      "Epoch 69/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.0930 - mean_cat_acc: 0.6841Epoch 00068: val_loss improved from 5.25274 to 5.24542, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.0906 - mean_cat_acc: 0.6843 - val_loss: 5.2454 - val_mean_cat_acc: 0.6756\n",
      "Epoch 70/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.0918 - mean_cat_acc: 0.6841Epoch 00069: val_loss improved from 5.24542 to 5.24122, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.0864 - mean_cat_acc: 0.6844 - val_loss: 5.2412 - val_mean_cat_acc: 0.6756\n",
      "Epoch 71/200\n",
      "5350/5362 [============================>.] - ETA: 0s - loss: 5.0886 - mean_cat_acc: 0.6843Epoch 00070: val_loss improved from 5.24122 to 5.24036, saving model to /tmp/best_0.12766833622653695.h5\n",
      "5362/5362 [==============================] - 8s - loss: 5.0863 - mean_cat_acc: 0.6844 - val_loss: 5.2404 - val_mean_cat_acc: 0.6756\n",
      "Epoch 72/200\n",
      "5000/5362 [==========================>...] - ETA: 0s - loss: 5.0709 - mean_cat_acc: 0.6854"
     ]
    }
   ],
   "source": [
    "conv_net.fit(x_data, y_data,\n",
    "             nb_epoch=num_epochs,\n",
    "             batch_size=batch_size,\n",
    "             validate=validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History saved to: models/dnabind/300_50_200_50_DeepCoFAM.history\n",
      "Model saved to: models/dnabind/300_50_200_50_DeepCoFAM.model\n"
     ]
    }
   ],
   "source": [
    "handle = Handle(batch_size=batch_size,\n",
    "                filter_length=filter_length,\n",
    "                filters=filters,\n",
    "                program='CoMET',\n",
    "                data_id=data_id)\n",
    "handle.model = 'DeepCoFAM'\n",
    "conv_net.save_train_history(handle)\n",
    "conv_net.save_model_to_file(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
